{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a766a2",
   "metadata": {},
   "source": [
    "# MLflow Integration Demo\n",
    "\n",
    "This notebook demonstrates how to use MLflow for experiment tracking, model comparison, and model registry in the GitHub Activity Forecasting project.\n",
    "\n",
    "## MLflow Components Covered:\n",
    "1. **Experiment Tracking** - Query runs and compare metrics\n",
    "2. **Parameter Analysis** - Compare hyperparameters across runs\n",
    "3. **Model Registry** - Access registered models\n",
    "4. **Artifact Management** - Load models and artifacts\n",
    "5. **Visualization** - Compare model performance\n",
    "\n",
    "## Prerequisites:\n",
    "```bash\n",
    "# Start MLflow UI (in separate terminal)\n",
    "mlflow ui --backend-store-uri mlruns\n",
    "# Access at: http://localhost:5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba169ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Initialize MLflow client\n",
    "mlflow.set_tracking_uri('mlruns')\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"‚úì MLflow client initialized\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408f7fe",
   "metadata": {},
   "source": [
    "## 1. List All Experiments\n",
    "\n",
    "MLflow organizes runs into experiments. Each experiment groups related model training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "experiments = client.search_experiments()\n",
    "\n",
    "print(f\"Total Experiments: {len(experiments)}\\n\")\n",
    "for exp in experiments:\n",
    "    print(f\"üìä {exp.name}\")\n",
    "    print(f\"   ID: {exp.experiment_id}\")\n",
    "    print(f\"   Artifact Location: {exp.artifact_location}\")\n",
    "    print(f\"   Lifecycle Stage: {exp.lifecycle_stage}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb08743",
   "metadata": {},
   "source": [
    "## 2. Query Forecasting Experiment Runs\n",
    "\n",
    "Retrieve all training runs from the forecasting experiment and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecasting experiment\n",
    "forecast_exp = client.get_experiment_by_name('forecasting-models')\n",
    "\n",
    "if forecast_exp:\n",
    "    # Search runs in this experiment\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[forecast_exp.experiment_id],\n",
    "        order_by=['metrics.best_dev_loss ASC'],\n",
    "        max_results=20\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(runs)} forecasting runs\\n\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    run_data = []\n",
    "    for run in runs:\n",
    "        run_data.append({\n",
    "            'run_id': run.info.run_id,\n",
    "            'run_name': run.data.tags.get('mlflow.runName', 'N/A'),\n",
    "            'model_type': run.data.params.get('model_type', 'N/A'),\n",
    "            'hidden_size': int(run.data.params.get('hidden_size', 0)),\n",
    "            'num_layers': int(run.data.params.get('num_layers', 0)),\n",
    "            'best_dev_loss': run.data.metrics.get('best_dev_loss', np.nan),\n",
    "            'final_train_loss': run.data.metrics.get('final_train_loss', np.nan),\n",
    "            'training_time_min': run.data.metrics.get('training_time_minutes', np.nan),\n",
    "            'start_time': pd.to_datetime(run.info.start_time, unit='ms')\n",
    "        })\n",
    "    \n",
    "    df_runs = pd.DataFrame(run_data)\n",
    "    df_runs = df_runs.sort_values('best_dev_loss')\n",
    "    \n",
    "    print(\"\\nüèÜ Top 5 Forecasting Models by Validation Loss:\")\n",
    "    print(df_runs.head()[['run_name', 'model_type', 'hidden_size', 'best_dev_loss', 'training_time_min']])\n",
    "else:\n",
    "    print(\"No forecasting experiment found. Run training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef049c74",
   "metadata": {},
   "source": [
    "## 3. Visualize Model Comparison\n",
    "\n",
    "Compare different models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c526589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_runs' in locals() and not df_runs.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Validation Loss by Model Type\n",
    "    df_plot = df_runs.dropna(subset=['best_dev_loss'])\n",
    "    if not df_plot.empty:\n",
    "        sns.barplot(data=df_plot.head(10), x='run_name', y='best_dev_loss', \n",
    "                   hue='model_type', ax=axes[0])\n",
    "        axes[0].set_title('Best Validation Loss by Model', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Run Name')\n",
    "        axes[0].set_ylabel('Validation Loss (MSE)')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title='Model Type')\n",
    "    \n",
    "    # Plot 2: Training Time vs Performance\n",
    "    df_plot2 = df_runs.dropna(subset=['best_dev_loss', 'training_time_min'])\n",
    "    if not df_plot2.empty:\n",
    "        for model_type in df_plot2['model_type'].unique():\n",
    "            data = df_plot2[df_plot2['model_type'] == model_type]\n",
    "            axes[1].scatter(data['training_time_min'], data['best_dev_loss'], \n",
    "                          label=model_type, s=100, alpha=0.6)\n",
    "        axes[1].set_title('Training Time vs Performance', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Training Time (minutes)')\n",
    "        axes[1].set_ylabel('Validation Loss (MSE)')\n",
    "        axes[1].legend(title='Model Type')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0540ff",
   "metadata": {},
   "source": [
    "## 4. Query Classification Experiment Runs\n",
    "\n",
    "Analyze activity classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd94b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification experiment\n",
    "class_exp = client.get_experiment_by_name('activity-classification')\n",
    "\n",
    "if class_exp:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[class_exp.experiment_id],\n",
    "        order_by=['metrics.test_f1 DESC'],\n",
    "        max_results=20\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(runs)} classification runs\\n\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    class_data = []\n",
    "    for run in runs:\n",
    "        class_data.append({\n",
    "            'run_id': run.info.run_id,\n",
    "            'model_type': run.data.params.get('model_type', 'N/A'),\n",
    "            'precision': run.data.metrics.get('test_precision', np.nan),\n",
    "            'recall': run.data.metrics.get('test_recall', np.nan),\n",
    "            'f1': run.data.metrics.get('test_f1', np.nan),\n",
    "            'roc_auc': run.data.metrics.get('test_roc_auc', np.nan),\n",
    "            'pr_auc': run.data.metrics.get('test_pr_auc', np.nan)\n",
    "        })\n",
    "    \n",
    "    df_class = pd.DataFrame(class_data)\n",
    "    \n",
    "    print(\"\\nüèÜ Classification Models Performance:\")\n",
    "    print(df_class[['model_type', 'precision', 'recall', 'f1', 'roc_auc']].round(4))\n",
    "else:\n",
    "    print(\"No classification experiment found. Run training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bee20",
   "metadata": {},
   "source": [
    "## 5. Visualize Classification Metrics\n",
    "\n",
    "Compare classification models across multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0393350",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_class' in locals() and not df_class.empty:\n",
    "    # Prepare data for radar chart\n",
    "    metrics = ['precision', 'recall', 'f1', 'roc_auc', 'pr_auc']\n",
    "    df_plot = df_class[['model_type'] + metrics].dropna()\n",
    "    \n",
    "    if not df_plot.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, (idx, row) in enumerate(df_plot.iterrows()):\n",
    "            values = [row[m] for m in metrics]\n",
    "            ax.bar(x + i * width, values, width, label=row['model_type'])\n",
    "        \n",
    "        ax.set_xlabel('Metrics', fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontweight='bold')\n",
    "        ax.set_title('Classification Model Comparison', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x + width)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.legend(title='Model Type')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No classification data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4577dcbc",
   "metadata": {},
   "source": [
    "## 6. Access Model Registry\n",
    "\n",
    "View registered models and their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all registered models\n",
    "registered_models = client.search_registered_models()\n",
    "\n",
    "print(f\"Total Registered Models: {len(registered_models)}\\n\")\n",
    "\n",
    "for model in registered_models:\n",
    "    print(f\"üì¶ {model.name}\")\n",
    "    print(f\"   Description: {model.description}\")\n",
    "    print(f\"   Latest Versions:\")\n",
    "    \n",
    "    # Get model versions\n",
    "    versions = client.search_model_versions(f\"name='{model.name}'\")\n",
    "    for version in versions[:3]:  # Show top 3 versions\n",
    "        print(f\"     v{version.version} - Stage: {version.current_stage}\")\n",
    "        print(f\"     Run ID: {version.run_id}\")\n",
    "        print(f\"     Source: {version.source}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364bb44",
   "metadata": {},
   "source": [
    "## 7. Load a Model from Registry\n",
    "\n",
    "Load a production model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a forecasting model\n",
    "try:\n",
    "    # Load latest version of LSTM forecaster\n",
    "    model_name = \"forecaster-lstm\"\n",
    "    model_uri = f\"models:/{model_name}/latest\"\n",
    "    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "    \n",
    "    print(f\"‚úì Model loaded successfully!\")\n",
    "    print(f\"Model type: {type(loaded_model)}\")\n",
    "    print(f\"\\nModel can now be used for predictions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model not found: {e}\")\n",
    "    print(\"Run training with --model lstm first to register a model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf5295",
   "metadata": {},
   "source": [
    "## 8. Compare Specific Runs\n",
    "\n",
    "Deep dive into comparing two or more specific runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_runs' in locals() and len(df_runs) >= 2:\n",
    "    # Get top 2 runs\n",
    "    top_runs = df_runs.head(2)\n",
    "    \n",
    "    print(\"Comparing Top 2 Runs:\\n\")\n",
    "    \n",
    "    for idx, row in top_runs.iterrows():\n",
    "        run_id = row['run_id']\n",
    "        run = client.get_run(run_id)\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Run: {row['run_name']}\")\n",
    "        print(f\"Run ID: {run_id}\")\n",
    "        print(f\"\\nParameters:\")\n",
    "        for key, value in sorted(run.data.params.items()):\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nMetrics:\")\n",
    "        for key, value in sorted(run.data.metrics.items()):\n",
    "            print(f\"  {key}: {value:.6f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Not enough runs available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3a337",
   "metadata": {},
   "source": [
    "## 9. Get Best Model Across All Experiments\n",
    "\n",
    "Find the best performing model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search across all experiments\n",
    "all_experiments = client.search_experiments()\n",
    "all_exp_ids = [exp.experiment_id for exp in all_experiments]\n",
    "\n",
    "# Find best forecasting run\n",
    "best_runs = client.search_runs(\n",
    "    experiment_ids=all_exp_ids,\n",
    "    filter_string=\"metrics.best_dev_loss > 0\",\n",
    "    order_by=['metrics.best_dev_loss ASC'],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if best_runs:\n",
    "    best_run = best_runs[0]\n",
    "    print(\"üèÜ BEST FORECASTING MODEL:\\n\")\n",
    "    print(f\"Run ID: {best_run.info.run_id}\")\n",
    "    print(f\"Run Name: {best_run.data.tags.get('mlflow.runName')}\")\n",
    "    print(f\"Model Type: {best_run.data.params.get('model_type')}\")\n",
    "    print(f\"Best Validation Loss: {best_run.data.metrics.get('best_dev_loss'):.6f}\")\n",
    "    print(f\"\\nKey Parameters:\")\n",
    "    key_params = ['hidden_size', 'num_layers', 'dropout', 'learning_rate']\n",
    "    for param in key_params:\n",
    "        if param in best_run.data.params:\n",
    "            print(f\"  {param}: {best_run.data.params[param]}\")\n",
    "else:\n",
    "    print(\"No runs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1488a8",
   "metadata": {},
   "source": [
    "## 10. Export Results for Presentation\n",
    "\n",
    "Create summary tables for project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = []\n",
    "\n",
    "if 'df_runs' in locals() and not df_runs.empty:\n",
    "    for _, row in df_runs.head(5).iterrows():\n",
    "        summary.append({\n",
    "            'Experiment': 'Forecasting',\n",
    "            'Model': row['model_type'],\n",
    "            'Config': f\"h={row['hidden_size']}, l={row['num_layers']}\",\n",
    "            'Metric': 'Best Val Loss',\n",
    "            'Value': f\"{row['best_dev_loss']:.6f}\",\n",
    "            'Time (min)': f\"{row['training_time_min']:.2f}\"\n",
    "        })\n",
    "\n",
    "if 'df_class' in locals() and not df_class.empty:\n",
    "    for _, row in df_class.iterrows():\n",
    "        summary.append({\n",
    "            'Experiment': 'Classification',\n",
    "            'Model': row['model_type'],\n",
    "            'Config': 'default',\n",
    "            'Metric': 'F1-Score',\n",
    "            'Value': f\"{row['f1']:.4f}\",\n",
    "            'Time (min)': 'N/A'\n",
    "        })\n",
    "\n",
    "if summary:\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    print(\"\\nüìä MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = 'mlflow_results_summary.csv'\n",
    "    df_summary.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úì Summary exported to: {output_path}\")\n",
    "else:\n",
    "    print(\"No data available for summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38f1f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### MLflow Benefits Demonstrated:\n",
    "\n",
    "1. **Experiment Tracking**: All training runs are automatically logged\n",
    "2. **Model Comparison**: Easy comparison across hyperparameters and architectures\n",
    "3. **Model Registry**: Centralized model versioning and stage management\n",
    "4. **Reproducibility**: Complete parameter and metric tracking\n",
    "5. **Visualization**: Built-in UI and programmatic access for analysis\n",
    "\n",
    "### For Presentation:\n",
    "\n",
    "- Show MLflow UI: `mlflow ui --backend-store-uri mlruns`\n",
    "- Highlight experiment comparison charts\n",
    "- Demonstrate model loading from registry\n",
    "- Emphasize reproducibility and tracking benefits\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Train multiple models with different hyperparameters\n",
    "2. Compare results in MLflow UI\n",
    "3. Promote best model to Production stage\n",
    "4. Use registered models for deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
