{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9578696",
   "metadata": {},
   "source": [
    "# Repository Activity Forecasting Experiments\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Rolling time-series cross-validation**\n",
    "2. **Autoregressive forecasting** with multiple models\n",
    "3. **Binary classification** metrics (active/inactive)\n",
    "4. **Visualization** of predictions vs actuals\n",
    "\n",
    "## Rolling Evaluation Strategy\n",
    "\n",
    "```\n",
    "Train: 2015-01 → 2018-06  |  Predict: 2018-07 → 2018-09  (Q3 2018)\n",
    "Train: 2015-01 → 2018-09  |  Predict: 2018-10 → 2018-12  (Q4 2018)\n",
    "Train: 2015-01 → 2018-12  |  Predict: 2019-01 → 2019-03  (Q1 2019)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb68947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../models')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score\n",
    "\n",
    "from forecaster import (\n",
    "    NaiveForecaster, MovingAverageForecaster,\n",
    "    LSTMForecaster, GRUForecaster,\n",
    "    create_sequences\n",
    ")\n",
    "\n",
    "# Settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e57c4",
   "metadata": {},
   "source": [
    "## 1. Load Labeled Quarterly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = '../data/processed/quarters_labeled.parquet'\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(data_path)\n",
    "    print(f\"✅ Loaded {len(df):,} records\")\n",
    "    print(f\"   Repositories: {df['repo_id'].nunique():,}\")\n",
    "    print(f\"   Date range: {df['quarter_start'].min()} to {df['quarter_end'].max()}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Data not found: {data_path}\")\n",
    "    print(\"   Run: python preprocessing/label_activity.py\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381048ce",
   "metadata": {},
   "source": [
    "## 2. Define Metric Columns and Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c227aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Identify metric columns to forecast\n",
    "    metric_cols = [col for col in df.columns \n",
    "                   if pd.api.types.is_numeric_dtype(df[col]) \n",
    "                   and col not in ['year', 'quarter', 'quarter_index', \n",
    "                                   'quarters_since_creation', 'total_quarters',\n",
    "                                   'activity_score', 'is_active']]\n",
    "    \n",
    "    print(f\"Metrics to forecast: {metric_cols}\")\n",
    "    \n",
    "    # Sequence length (number of past quarters to use)\n",
    "    SEQUENCE_LENGTH = 4\n",
    "    print(f\"Sequence length: {SEQUENCE_LENGTH} quarters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9f131",
   "metadata": {},
   "source": [
    "## 3. Simple Train/Test Split Example\n",
    "\n",
    "First, let's do a simple example: train on all data before 2019, predict 2019 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Split by date\n",
    "    split_date = '2019-01-01'\n",
    "    \n",
    "    df_train = df[df['quarter_start'] < split_date]\n",
    "    df_test = df[df['quarter_start'] >= split_date]\n",
    "    \n",
    "    print(f\"Train: {len(df_train):,} records ({df_train['quarter_start'].min()} to {df_train['quarter_end'].max()})\")\n",
    "    print(f\"Test:  {len(df_test):,} records ({df_test['quarter_start'].min()} to {df_test['quarter_end'].max()})\")\n",
    "    \n",
    "    # Create sequences\n",
    "    print(\"\\nCreating training sequences...\")\n",
    "    X_train, y_train, train_repo_ids = create_sequences(\n",
    "        df_train, SEQUENCE_LENGTH, metric_cols\n",
    "    )\n",
    "    \n",
    "    print(f\"Training sequences: {X_train.shape}\")\n",
    "    print(f\"Training targets: {y_train.shape}\")\n",
    "    \n",
    "    print(\"\\nCreating test sequences...\")\n",
    "    X_test, y_test, test_repo_ids = create_sequences(\n",
    "        df_test, SEQUENCE_LENGTH, metric_cols\n",
    "    )\n",
    "    \n",
    "    print(f\"Test sequences: {X_test.shape}\")\n",
    "    print(f\"Test targets: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c744b",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b351998",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and X_train is not None:\n",
    "    results = {}\n",
    "    \n",
    "    # Naive baseline\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NAIVE FORECASTER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    naive_model = NaiveForecaster()\n",
    "    naive_model.fit(X_train, y_train)\n",
    "    naive_metrics = naive_model.evaluate(X_test, y_test)\n",
    "    results['naive'] = naive_metrics\n",
    "    \n",
    "    print(f\"MSE:  {naive_metrics['mse']:.6f}\")\n",
    "    print(f\"MAE:  {naive_metrics['mae']:.6f}\")\n",
    "    print(f\"RMSE: {naive_metrics['rmse']:.6f}\")\n",
    "    \n",
    "    # Moving Average\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MOVING AVERAGE FORECASTER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ma_model = MovingAverageForecaster(window_size=3)\n",
    "    ma_model.fit(X_train, y_train)\n",
    "    ma_metrics = ma_model.evaluate(X_test, y_test)\n",
    "    results['moving_average'] = ma_metrics\n",
    "    \n",
    "    print(f\"MSE:  {ma_metrics['mse']:.6f}\")\n",
    "    print(f\"MAE:  {ma_metrics['mae']:.6f}\")\n",
    "    print(f\"RMSE: {ma_metrics['rmse']:.6f}\")\n",
    "    \n",
    "    # LSTM (small model for demo)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LSTM FORECASTER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lstm_model = LSTMForecaster(\n",
    "        input_size=len(metric_cols),\n",
    "        hidden_size=32,\n",
    "        num_layers=1,\n",
    "        epochs=20,\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    # Sample smaller subset for quick training (remove for full training)\n",
    "    sample_size = min(5000, len(X_train))\n",
    "    lstm_model.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    lstm_metrics = lstm_model.evaluate(X_test, y_test)\n",
    "    results['lstm'] = lstm_metrics\n",
    "    \n",
    "    print(f\"MSE:  {lstm_metrics['mse']:.6f}\")\n",
    "    print(f\"MAE:  {lstm_metrics['mae']:.6f}\")\n",
    "    print(f\"RMSE: {lstm_metrics['rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea7c39",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356999de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in locals():\n",
    "    # Create comparison table\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'MSE': [results[m]['mse'] for m in results],\n",
    "        'MAE': [results[m]['mae'] for m in results],\n",
    "        'RMSE': [results[m]['rmse'] for m in results]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    display(comparison)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for i, metric in enumerate(['MSE', 'MAE', 'RMSE']):\n",
    "        axes[i].bar(comparison['Model'], comparison[metric], alpha=0.7)\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].set_title(f'{metric} Comparison')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beab911",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions for Sample Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4888a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'lstm_model' in locals() and X_test is not None:\n",
    "    # Get predictions from best model\n",
    "    predictions = lstm_model.predict(X_test[:100])  # Sample\n",
    "    actuals = y_test[:100]\n",
    "    \n",
    "    # Plot first metric for sample repos\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(6):\n",
    "        if i < len(predictions):\n",
    "            # Plot all metrics for this sample\n",
    "            x = np.arange(len(metric_cols))\n",
    "            width = 0.35\n",
    "            \n",
    "            axes[i].bar(x - width/2, actuals[i], width, label='Actual', alpha=0.7)\n",
    "            axes[i].bar(x + width/2, predictions[i], width, label='Predicted', alpha=0.7)\n",
    "            axes[i].set_xlabel('Metric')\n",
    "            axes[i].set_ylabel('Value')\n",
    "            axes[i].set_title(f'Sample {i+1}')\n",
    "            axes[i].set_xticks(x)\n",
    "            axes[i].set_xticklabels([m[:10] for m in metric_cols], rotation=45, ha='right')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aada29",
   "metadata": {},
   "source": [
    "## 7. Binary Classification: Active vs Inactive\n",
    "\n",
    "Convert forecasts to binary labels and compute classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predictions' in locals() and 'actuals' in locals():\n",
    "    # Load threshold from labeling\n",
    "    import json\n",
    "    \n",
    "    try:\n",
    "        with open('../config/threshold_config.json', 'r') as f:\n",
    "            threshold_config = json.load(f)\n",
    "            threshold = threshold_config['threshold']\n",
    "    except:\n",
    "        # Use default threshold (median activity score)\n",
    "        threshold = df['activity_score'].median()\n",
    "    \n",
    "    print(f\"Using activity threshold: {threshold:.4f}\")\n",
    "    \n",
    "    # Compute activity scores for predictions and actuals\n",
    "    # Simple heuristic: sum of log-transformed metrics\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "    \n",
    "    pred_scores = np.log1p(predictions).sum(axis=1)\n",
    "    actual_scores = np.log1p(actuals).sum(axis=1)\n",
    "    \n",
    "    # Binary labels\n",
    "    pred_labels = (pred_scores >= threshold).astype(int)\n",
    "    actual_labels = (actual_scores >= threshold).astype(int)\n",
    "    \n",
    "    # Classification metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "    \n",
    "    precision = precision_score(actual_labels, pred_labels, zero_division=0)\n",
    "    recall = recall_score(actual_labels, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(actual_labels, pred_labels, zero_division=0)\n",
    "    accuracy = accuracy_score(actual_labels, pred_labels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BINARY CLASSIFICATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, pred_labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Inactive', 'Active'],\n",
    "                yticklabels=['Inactive', 'Active'])\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Confusion Matrix: Active/Inactive Classification')\n",
    "    plt.savefig('../data/processed/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10453241",
   "metadata": {},
   "source": [
    "## 8. Rolling Cross-Validation (Simplified)\n",
    "\n",
    "Demonstrate one step of rolling evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f61729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROLLING CROSS-VALIDATION EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define rolling windows\n",
    "    windows = [\n",
    "        ('2015-01-01', '2018-06-30', '2018-07-01', '2018-09-30'),  # Q3 2018\n",
    "        ('2015-01-01', '2018-09-30', '2018-10-01', '2018-12-31'),  # Q4 2018\n",
    "    ]\n",
    "    \n",
    "    rolling_results = []\n",
    "    \n",
    "    for train_start, train_end, test_start, test_end in windows:\n",
    "        print(f\"\\nTrain: {train_start} → {train_end}\")\n",
    "        print(f\"Test:  {test_start} → {test_end}\")\n",
    "        \n",
    "        # Split data\n",
    "        df_train_roll = df[\n",
    "            (df['quarter_start'] >= train_start) & \n",
    "            (df['quarter_end'] <= train_end)\n",
    "        ]\n",
    "        df_test_roll = df[\n",
    "            (df['quarter_start'] >= test_start) & \n",
    "            (df['quarter_end'] <= test_end)\n",
    "        ]\n",
    "        \n",
    "        if len(df_train_roll) == 0 or len(df_test_roll) == 0:\n",
    "            print(\"  ⚠️ Insufficient data for this window\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Train samples: {len(df_train_roll):,}\")\n",
    "        print(f\"  Test samples:  {len(df_test_roll):,}\")\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train_roll, y_train_roll, _ = create_sequences(\n",
    "            df_train_roll, SEQUENCE_LENGTH, metric_cols\n",
    "        )\n",
    "        X_test_roll, y_test_roll, _ = create_sequences(\n",
    "            df_test_roll, SEQUENCE_LENGTH, metric_cols\n",
    "        )\n",
    "        \n",
    "        if len(X_train_roll) == 0 or len(X_test_roll) == 0:\n",
    "            print(\"  ⚠️ Could not create sequences\")\n",
    "            continue\n",
    "        \n",
    "        # Train simple model (Naive for speed)\n",
    "        model = NaiveForecaster()\n",
    "        model.fit(X_train_roll, y_train_roll)\n",
    "        metrics = model.evaluate(X_test_roll, y_test_roll)\n",
    "        \n",
    "        print(f\"  MSE: {metrics['mse']:.6f}, MAE: {metrics['mae']:.6f}\")\n",
    "        \n",
    "        rolling_results.append({\n",
    "            'test_period': f\"{test_start} to {test_end}\",\n",
    "            'mse': metrics['mse'],\n",
    "            'mae': metrics['mae'],\n",
    "            'rmse': metrics['rmse']\n",
    "        })\n",
    "    \n",
    "    if rolling_results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ROLLING RESULTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        rolling_df = pd.DataFrame(rolling_results)\n",
    "        display(rolling_df)\n",
    "        \n",
    "        print(f\"\\nAverage MSE across windows: {rolling_df['mse'].mean():.6f}\")\n",
    "        print(f\"Average MAE across windows: {rolling_df['mae'].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc881b92",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Loading labeled quarterly data\n",
    "2. ✅ Creating time series sequences\n",
    "3. ✅ Training multiple forecasting models (Naive, MA, LSTM)\n",
    "4. ✅ Computing forecast metrics (MSE, MAE, RMSE)\n",
    "5. ✅ Binary classification metrics (Precision, Recall, F1)\n",
    "6. ✅ Rolling cross-validation example\n",
    "7. ✅ Visualizations of predictions vs actuals\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Tune hyperparameters for LSTM/GRU models\n",
    "- Implement full rolling evaluation pipeline\n",
    "- Try more sophisticated models (Transformer, TCN)\n",
    "- Add repository-specific features\n",
    "- Analyze per-metric forecasting performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
