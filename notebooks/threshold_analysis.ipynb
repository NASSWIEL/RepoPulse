{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a9645b",
   "metadata": {},
   "source": [
    "# Threshold Analysis for Repository Activity Labeling\n",
    "\n",
    "This notebook demonstrates the methodology used to select optimal thresholds for labeling repositories as active or inactive.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Compute weighted activity score** using multiple metrics\n",
    "2. **Create pseudo-labels** from extreme percentiles (top 25% = active, bottom 25% = inactive)\n",
    "3. **Evaluate different threshold selection methods**:\n",
    "   - F1-maximization\n",
    "   - Youden's J statistic\n",
    "   - Precision/Recall tradeoffs\n",
    "4. **Visualize precision-recall and ROC curves**\n",
    "5. **Analyze activity score distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd634ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, \n",
    "    roc_curve, \n",
    "    auc, \n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca787eb",
   "metadata": {},
   "source": [
    "## 1. Load Quarterly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c08e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quarterly aggregated data\n",
    "data_path = '../data/processed/quarters.parquet'\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(data_path)\n",
    "    print(f\"✅ Loaded {len(df):,} records\")\n",
    "    print(f\"   Unique repositories: {df['repo_id'].nunique():,}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Data file not found: {data_path}\")\n",
    "    print(\"   Please run: python preprocessing/aggregate_quarters.py\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684adffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample\n",
    "if df is not None:\n",
    "    display(df.head())\n",
    "    print(\"\\nData types:\")\n",
    "    display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc87cf9",
   "metadata": {},
   "source": [
    "## 2. Compute Activity Scores\n",
    "\n",
    "### Metric Weights (Rationale)\n",
    "\n",
    "We use weighted combination of metrics:\n",
    "\n",
    "- **Commits (weight=1.0)**: Direct indicator of code activity\n",
    "- **Pull Requests (weight=2.0)**: High-value collaboration, code review\n",
    "- **Issues (weight=0.5)**: Community engagement, bug reports\n",
    "- **Stars (weight=0.1)**: Interest signal, but not active contribution\n",
    "- **Forks (weight=0.3)**: Potential for external development\n",
    "\n",
    "Scores are computed using log-transform to handle skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'commits': 1.0,\n",
    "        'commit': 1.0,\n",
    "        'pull_requests': 2.0,\n",
    "        'pulls': 2.0,\n",
    "        'pr': 2.0,\n",
    "        'issues': 0.5,\n",
    "        'issue': 0.5,\n",
    "        'stars': 0.1,\n",
    "        'star': 0.1,\n",
    "        'forks': 0.3,\n",
    "        'fork': 0.3,\n",
    "    }\n",
    "    \n",
    "    # Identify available metric columns\n",
    "    available_metrics = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        for metric_key, weight in weights.items():\n",
    "            if metric_key in col_lower and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                available_metrics[col] = weight\n",
    "                break\n",
    "    \n",
    "    print(f\"Available metrics: {list(available_metrics.keys())}\")\n",
    "    print(f\"Weights: {available_metrics}\")\n",
    "    \n",
    "    # Compute weighted score\n",
    "    df['activity_score'] = 0.0\n",
    "    for col, weight in available_metrics.items():\n",
    "        normalized = np.log1p(df[col].fillna(0))\n",
    "        df['activity_score'] += weight * normalized\n",
    "    \n",
    "    print(f\"\\nActivity score statistics:\")\n",
    "    print(df['activity_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2692c1",
   "metadata": {},
   "source": [
    "## 3. Visualize Activity Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(df['activity_score'], bins=100, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Activity Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Activity Scores')\n",
    "    axes[0].axvline(df['activity_score'].median(), color='red', linestyle='--', label='Median')\n",
    "    axes[0].axvline(df['activity_score'].mean(), color='green', linestyle='--', label='Mean')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(df['activity_score'], vert=True)\n",
    "    axes[1].set_ylabel('Activity Score')\n",
    "    axes[1].set_title('Activity Score Box Plot')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/activity_score_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print percentiles\n",
    "    percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "    print(\"\\nActivity Score Percentiles:\")\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(df['activity_score'], p)\n",
    "        print(f\"  {p}th: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b9d6a",
   "metadata": {},
   "source": [
    "## 4. Create Pseudo-Labels for Validation\n",
    "\n",
    "We use a semi-supervised approach:\n",
    "- **Top 25%** of scores → definitely active (label=1)\n",
    "- **Bottom 25%** of scores → definitely inactive (label=0)\n",
    "- Middle 50% → ambiguous (not used for threshold tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    scores = df['activity_score'].values\n",
    "    \n",
    "    # Define percentile thresholds\n",
    "    top_percentile = np.percentile(scores, 75)\n",
    "    bottom_percentile = np.percentile(scores, 25)\n",
    "    \n",
    "    print(f\"Top 25% threshold: {top_percentile:.4f}\")\n",
    "    print(f\"Bottom 25% threshold: {bottom_percentile:.4f}\")\n",
    "    \n",
    "    # Create validation set (only extreme values)\n",
    "    validation_mask = (scores >= top_percentile) | (scores <= bottom_percentile)\n",
    "    val_scores = scores[validation_mask]\n",
    "    val_labels = (scores[validation_mask] >= top_percentile).astype(int)\n",
    "    \n",
    "    print(f\"\\nValidation set size: {len(val_scores):,}\")\n",
    "    print(f\"  Active (label=1): {val_labels.sum():,}\")\n",
    "    print(f\"  Inactive (label=0): {(1 - val_labels).sum():,}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(scores[scores <= bottom_percentile], bins=50, alpha=0.6, label='Inactive', color='red')\n",
    "    plt.hist(scores[scores >= top_percentile], bins=50, alpha=0.6, label='Active', color='green')\n",
    "    plt.axvline(bottom_percentile, color='red', linestyle='--', linewidth=2)\n",
    "    plt.axvline(top_percentile, color='green', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Activity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Pseudo-Labels: Active vs Inactive')\n",
    "    plt.legend()\n",
    "    plt.savefig('../data/processed/pseudo_labels_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57c010",
   "metadata": {},
   "source": [
    "## 5. Compute Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Compute precision-recall curve\n",
    "    precision, recall, thresholds_pr = precision_recall_curve(val_labels, val_scores)\n",
    "    \n",
    "    # Compute F1 scores\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # Find optimal threshold (max F1)\n",
    "    best_idx = np.argmax(f1_scores[:-1])\n",
    "    optimal_threshold_f1 = thresholds_pr[best_idx]\n",
    "    optimal_f1 = f1_scores[best_idx]\n",
    "    optimal_precision = precision[best_idx]\n",
    "    optimal_recall = recall[best_idx]\n",
    "    \n",
    "    print(f\"Optimal Threshold (max F1): {optimal_threshold_f1:.4f}\")\n",
    "    print(f\"  F1 Score: {optimal_f1:.4f}\")\n",
    "    print(f\"  Precision: {optimal_precision:.4f}\")\n",
    "    print(f\"  Recall: {optimal_recall:.4f}\")\n",
    "    \n",
    "    # Plot PR curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, linewidth=2, label='PR Curve')\n",
    "    plt.scatter([optimal_recall], [optimal_precision], s=200, c='red', marker='*', \n",
    "                label=f'Optimal (F1={optimal_f1:.3f})', zorder=5)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../data/processed/precision_recall_curve.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d15e7",
   "metadata": {},
   "source": [
    "## 6. Compute ROC Curve and Youden's J Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds_roc = roc_curve(val_labels, val_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Compute Youden's J statistic\n",
    "    youden_j = tpr - fpr\n",
    "    best_idx_youden = np.argmax(youden_j)\n",
    "    optimal_threshold_youden = thresholds_roc[best_idx_youden]\n",
    "    optimal_youden = youden_j[best_idx_youden]\n",
    "    \n",
    "    print(f\"Optimal Threshold (Youden's J): {optimal_threshold_youden:.4f}\")\n",
    "    print(f\"  Youden's J: {optimal_youden:.4f}\")\n",
    "    print(f\"  TPR: {tpr[best_idx_youden]:.4f}\")\n",
    "    print(f\"  FPR: {fpr[best_idx_youden]:.4f}\")\n",
    "    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC={roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    plt.scatter([fpr[best_idx_youden]], [tpr[best_idx_youden]], s=200, c='red', marker='*',\n",
    "                label=f'Optimal (J={optimal_youden:.3f})', zorder=5)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../data/processed/roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e53ee",
   "metadata": {},
   "source": [
    "## 7. Compare Threshold Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Create comparison table\n",
    "    threshold_methods = {\n",
    "        'F1 Maximization': optimal_threshold_f1,\n",
    "        'Youden\\'s J': optimal_threshold_youden,\n",
    "        'Percentile (50th)': np.percentile(scores, 50),\n",
    "        'Percentile (75th)': np.percentile(scores, 75),\n",
    "    }\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for method_name, threshold in threshold_methods.items():\n",
    "        # Apply threshold to validation set\n",
    "        pred_labels = (val_scores >= threshold).astype(int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "        \n",
    "        precision_val = precision_score(val_labels, pred_labels, zero_division=0)\n",
    "        recall_val = recall_score(val_labels, pred_labels, zero_division=0)\n",
    "        f1_val = f1_score(val_labels, pred_labels, zero_division=0)\n",
    "        accuracy_val = accuracy_score(val_labels, pred_labels)\n",
    "        \n",
    "        # Apply to full dataset\n",
    "        active_pct = (scores >= threshold).mean() * 100\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'Method': method_name,\n",
    "            'Threshold': threshold,\n",
    "            'F1': f1_val,\n",
    "            'Precision': precision_val,\n",
    "            'Recall': recall_val,\n",
    "            'Accuracy': accuracy_val,\n",
    "            'Active %': active_pct\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    print(\"\\nThreshold Method Comparison:\")\n",
    "    display(comparison_df.style.format({\n",
    "        'Threshold': '{:.4f}',\n",
    "        'F1': '{:.4f}',\n",
    "        'Precision': '{:.4f}',\n",
    "        'Recall': '{:.4f}',\n",
    "        'Accuracy': '{:.4f}',\n",
    "        'Active %': '{:.2f}'\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e51ee5",
   "metadata": {},
   "source": [
    "## 8. Recommended Threshold\n",
    "\n",
    "Based on the analysis above, we recommend using the **F1-maximization** method as it provides a balanced tradeoff between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    recommended_threshold = optimal_threshold_f1\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RECOMMENDED THRESHOLD\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Threshold: {recommended_threshold:.4f}\")\n",
    "    print(f\"Method: F1 Maximization\")\n",
    "    print(f\"F1 Score: {optimal_f1:.4f}\")\n",
    "    print(f\"Precision: {optimal_precision:.4f}\")\n",
    "    print(f\"Recall: {optimal_recall:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save threshold to file\n",
    "    import json\n",
    "    threshold_config = {\n",
    "        'threshold': float(recommended_threshold),\n",
    "        'method': 'f1_maximization',\n",
    "        'f1_score': float(optimal_f1),\n",
    "        'precision': float(optimal_precision),\n",
    "        'recall': float(optimal_recall)\n",
    "    }\n",
    "    \n",
    "    with open('../config/threshold_config.json', 'w') as f:\n",
    "        json.dump(threshold_config, f, indent=2)\n",
    "    \n",
    "    print(\"\\n✅ Threshold configuration saved to: ../config/threshold_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ffcde",
   "metadata": {},
   "source": [
    "## 9. Visualize Final Labeling\n",
    "\n",
    "Apply the recommended threshold and visualize the resulting labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Apply threshold\n",
    "    df['is_active'] = (df['activity_score'] >= recommended_threshold).astype(int)\n",
    "    \n",
    "    # Statistics\n",
    "    active_count = df['is_active'].sum()\n",
    "    inactive_count = len(df) - active_count\n",
    "    active_pct = (active_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Active: {active_count:,} ({active_pct:.1f}%)\")\n",
    "    print(f\"Inactive: {inactive_count:,} ({100 - active_pct:.1f}%)\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Distribution with threshold\n",
    "    axes[0].hist(df[df['is_active'] == 0]['activity_score'], bins=50, alpha=0.6, \n",
    "                 label='Inactive', color='red')\n",
    "    axes[0].hist(df[df['is_active'] == 1]['activity_score'], bins=50, alpha=0.6, \n",
    "                 label='Active', color='green')\n",
    "    axes[0].axvline(recommended_threshold, color='black', linestyle='--', linewidth=2,\n",
    "                    label=f'Threshold={recommended_threshold:.2f}')\n",
    "    axes[0].set_xlabel('Activity Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Final Activity Labels')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[1].bar(['Inactive', 'Active'], [inactive_count, active_count], \n",
    "                color=['red', 'green'], alpha=0.6)\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Label Distribution')\n",
    "    for i, (label, count) in enumerate([('Inactive', inactive_count), ('Active', active_count)]):\n",
    "        pct = (count / len(df)) * 100\n",
    "        axes[1].text(i, count, f\"{count:,}\\n({pct:.1f}%)\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/final_labels.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35ad5c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Computation of weighted activity scores from multiple metrics\n",
    "2. ✅ Creation of pseudo-labels from extreme percentiles\n",
    "3. ✅ Evaluation of multiple threshold selection methods\n",
    "4. ✅ Recommendation of F1-maximization method\n",
    "5. ✅ Visualization of precision-recall and ROC curves\n",
    "6. ✅ Final labeling with selected threshold\n",
    "\n",
    "The selected threshold provides a balanced tradeoff between precision and recall, making it suitable for both forecasting and classification tasks downstream."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
