# Configuration for GitHub Repository Activity Forecasting Pipeline

# Data Paths
data:
  raw_data_path: "/info/raid-etu/m2/s2308975/big_data"
  aggregated_data_path: "/info/raid-etu/m2/s2405959/BigData/data/processed/quarterly_aggregated.parquet"
  labeled_data_path: "/info/raid-etu/m2/s2405959/BigData/data/processed/quarterly_labeled.parquet"
  processed_path: "data/processed"
  output_path: "/info/raid-etu/m2/s2405959/BigData/data/processed"

# Preprocessing
preprocessing:
  # Expanding-window training: Use all available history (starting from min_lookback)
  expanding_window: true  # If true, use progressive expanding windows; if false, use fixed lookback
  min_lookback: 2  # Minimum historical quarters required (used as starting window)
  max_lookback: null  # Maximum lookback (null = no limit, use all available history)
  imputation_strategy: "zero"  # zero, forward_fill, interpolate
  min_quarters: 2  # Minimum quarters required per repository
  date_range:
    start: null  # YYYY-MM-DD or null for all data
    end: null

# Activity Labeling
labeling:
  threshold_method: "f1"  # f1, youden, precision, recall
  validation_split: 0.2
  weights:
    commits: 1.0
    pull_requests: 2.0
    issues: 0.5
    stars: 0.1
    forks: 0.3

# Forecasting Models
forecasting:
  models:
    naive:
      enabled: true
    
    moving_average:
      enabled: true
      window_size: 3
    
    lstm:
      enabled: true
      hidden_size: 16  # Reduced from 64 to prevent overfitting (7K samples)
      num_layers: 1    # Reduced from 2 to simplify model
      dropout: 0.3     # Increased from 0.2 for regularization
      learning_rate: 0.001
      batch_size: 64   # Increased from 32 for better generalization
      epochs: 100      # Increased with early stopping (patience=15)
    
    gru:
      enabled: true
      hidden_size: 16  # Reduced from 64 to prevent overfitting
      num_layers: 1    # Reduced from 2 to simplify model  
      dropout: 0.3     # Increased from 0.2 for regularization
      learning_rate: 0.001
      batch_size: 64   # Increased from 32 for better generalization
      epochs: 100      # Increased with early stopping (patience=15)
    
    tcn:
      enabled: true
      num_channels: [32, 32, 32]  # 3 levels of temporal convolutions
      kernel_size: 3               # Kernel size for convolutions
      dropout: 0.2                 # Dropout for regularization
      learning_rate: 0.001
      batch_size: 64
      epochs: 100

# Classification
classification:
  model: "logistic"  # logistic, rf, gbm
  test_size: 0.2
  random_forest:
    n_estimators: 100
    max_depth: 10
  gradient_boosting:
    n_estimators: 100
    learning_rate: 0.1

# Evaluation
evaluation:
  metrics:
    - mse
    - mae
    - rmse
    - precision
    - recall
    - f1
    - roc_auc
    - pr_auc
  
  rolling_windows:
    - train_start: "2015-01-01"
      train_end: "2018-06-30"
      test_start: "2018-07-01"
      test_end: "2018-09-30"
    - train_start: "2015-01-01"
      train_end: "2018-09-30"
      test_start: "2018-10-01"
      test_end: "2018-12-31"

# Training
training:
  random_seed: 42
  device: "auto"  # auto, cuda, cpu
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001
  
  checkpoint:
    save_best_only: true
    save_frequency: 10  # epochs

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "logs/pipeline.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# MLflow Configuration - Experiment Tracking & Model Registry
mlflow:
  tracking_uri: "mlruns"  # Local directory for MLflow tracking
  experiment_name: "github-activity-forecasting"  # Main experiment name
  artifact_location: "mlruns/artifacts"  # Where to store model artifacts
  
  # Experiments for different model types
  experiments:
    forecasting: "forecasting-models"
    classification: "activity-classification"
    preprocessing: "data-preprocessing"
  
  # Model registry
  registry:
    enabled: true
    production_stage: "Production"
    staging_stage: "Staging"
  
  # Logging configuration
  log_system_metrics: true  # Log CPU, GPU, memory usage
  log_every_n_epoch: 1  # Log metrics every N epochs
  
  # Tags to track
  default_tags:
    project: "BigData-GitHub-Activity"
    team: "M2-DataScience"
    framework: "PyTorch"

# Output
output:
  export_csv: true
  save_plots: true
  save_metrics: true
  plot_format: "png"  # png, pdf, svg
